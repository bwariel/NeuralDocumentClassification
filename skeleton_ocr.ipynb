{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import os\n",
    "import re\n",
    "\n",
    "from typing import List, Tuple, Union, NewType, OrderedDict, Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.model_selection as model_selection\n",
    "import tqdm\n",
    "\n",
    "import ocr_input\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "InputText = NewType('InputText', Union[str, List[str]])\n",
    "Label = NewType('Label', int)\n",
    "DocumentRecord = NewType('DocumentRecord', Tuple[InputText, Label])\n",
    "Dataset = NewType('Dataset', List[DocumentRecord])\n",
    "\n",
    "Token = NewType('Token', str)\n",
    "Vocabulary = NewType('Vocabulary', OrderedDict[Token, int])\n",
    "\n",
    "class_names = ['email', 'form', 'handwritten', 'invoice', 'advertisement']\n",
    "num_classes = len(class_names)\n",
    "\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset() -> Dataset:\n",
    "    dataset_path = r\".\\dataset\\\\ocr\"\n",
    "\n",
    "    all_files = os.listdir(dataset_path)\n",
    "    doc_ocr_d = {file: content for file, content in tqdm.tqdm(zip(map(lambda f: os.path.splitext(f)[0], all_files), \n",
    "                                                                  map(ocr_input.parse_xml, map(lambda p: os.path.join(dataset_path, p), all_files))),\n",
    "                                                                  total=len(all_files))}\n",
    "\n",
    "    with open(r\".\\dataset\\label.txt\", \"r\") as fp:\n",
    "        label_d = {file: label for file, label in map(lambda line: line.split(','), fp.readlines())}\n",
    "\n",
    "    return [(doc_ocr_d[file], label) for file, label in label_d.items()]\n",
    "\n",
    "# TODO: use split label for this\n",
    "def split_dataset(dataset: Dataset, test_size: float = 0.2, random_seed: int = random_seed) -> Tuple[List[InputText], List[InputText], List[Label], List[Label]]:\n",
    "    return model_selection.train_test_split(*zip(*dataset), test_size=test_size, random_state=random_seed)\n",
    "    \n",
    "\n",
    "dataset = get_dataset()\n",
    "x_train, x_test, y_train, y_test = split_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))\n",
    "print(len(x_train), len(x_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive vocabulary counting: splitting on space character\n",
    "\n",
    "# Conventions:\n",
    "# index 0 is reserved for unknown tokens that will be mapped to `__UNK__`.\n",
    "# other special token come just after (eg. `__NUM__` for numbers).\n",
    "# other classic token are inserted in order for reverse dictionnary purpose.\n",
    "__UNK__ = '__UNK__'\n",
    "\n",
    "# always put __UNK__ first when redefining special char.\n",
    "DEFAULT_SPECIALS = [__UNK__]\n",
    "\n",
    "def unknown_wrapped(f):\n",
    "    def wrapped(word, vocabulary=None):\n",
    "        res = f(word)\n",
    "\n",
    "        if vocabulary is not None and res not in vocabulary:\n",
    "            return __UNK__\n",
    "        return res\n",
    "    \n",
    "    return wrapped\n",
    "\n",
    "\n",
    "@unknown_wrapped\n",
    "def word_to_token(word: str) -> Token:\n",
    "    return word\n",
    "\n",
    "\n",
    "def simple_preprocess(text: str) -> str:\n",
    "    text = re.sub(r\"([.?,!])\", r\" \\1 \", text)  # Add an extra space before punctuation (usefull in english)\n",
    "    return re.sub(r\"[^a-zA-Z0-9.?,!$£€/-+\\\\]\", \" \", text)  # Remove any non basic character\n",
    "\n",
    "\n",
    "def compute_vocabulary(input_text: List[InputText], max_size=1000, tokenize_f=word_to_token, specials=DEFAULT_SPECIALS, preprocess=simple_preprocess) -> Tuple[Vocabulary, Counter[Token]]:\n",
    "    word_occurences_d = collections.Counter(i for i in map(tokenize_f,                           # Convert each word to its token\n",
    "                                                           itertools.chain(*map(str.split,       # Split words on space character ` ` and flatten the iterables \n",
    "                                                                                map(preprocess,  # Preprocess the text before tokenization\n",
    "                                                                                    x_train)))\n",
    "                                            if i)                                                # Filter out empty strings \n",
    "    # compute special tokens\n",
    "    no_special = len(specials)\n",
    "\n",
    "    vocabulary = collections.OrderedDict([(word, i) for i, (word, _) in enumerate(word_occurences_d.most_common(max_size - no_special), no_special)])\n",
    "\n",
    "    for i, sp in enumerate(reversed(specials), 1):\n",
    "        vocabulary[sp] = no_special - i\n",
    "        vocabulary.move_to_end(sp, last=False)\n",
    "\n",
    "    return vocabulary, word_occurences_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vocabulary(vocabulary, n=1000):\n",
    "    plt.plot(list(range(n)), [i for _, i in vocabulary.most_common(n)])\n",
    "\n",
    "    plt.title(f\"Évolution du nombre d'occurrence des {n} tokens les plus fréquents\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_accumulated_vocabulary(vocabulary, n=1000):\n",
    "    total_tokens = sum(vocabulary.values()) / 100\n",
    "    plt.plot(list(range(n)), list(itertools.accumulate(i / total_tokens for _, i in vocabulary.most_common(n))))\n",
    "\n",
    "    plt.title(f\"Évolution du nombre d'occurrences cumulé des {n} tokens les plus fréquents rapporté au nombre total de tokens\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary, word_count = compute_vocabulary(x_train, max_size=10**4)\n",
    "plot_vocabulary(word_count, n=10000)\n",
    "plot_accumulated_vocabulary(word_count, n=10000)\n",
    "print(len(word_count))\n",
    "\n",
    "print(list(vocabulary.items())[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We must reduce vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@unknown_wrapped\n",
    "def word_to_token_reduced(word: str) -> Token:\n",
    "    return word.lower()\n",
    "\n",
    "\n",
    "vocabulary, word_count = compute_vocabulary(x_train, max_size=10**4, tokenize_f=word_to_token_reduced)\n",
    "plot_vocabulary(word_count, n=10000)\n",
    "plot_accumulated_vocabulary(word_count, n=10000)\n",
    "print(len(word_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First simple model: Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}