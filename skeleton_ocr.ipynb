{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import os\n",
    "import re\n",
    "\n",
    "from functools import partial\n",
    "from typing import List, Tuple, Union, NewType, OrderedDict, Counter, Iterator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.model_selection as model_selection  # TODO: remove and use predefined splits\n",
    "import tqdm\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "import ocr_input\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "InputText = NewType('InputText', Union[str, List[str]])\n",
    "Label = NewType('Label', int)\n",
    "DocumentRecord = NewType('DocumentRecord', Tuple[InputText, Label])\n",
    "Dataset = NewType('Dataset', List[DocumentRecord])\n",
    "\n",
    "Token = NewType('Token', str)\n",
    "Vocabulary = NewType('Vocabulary', OrderedDict[Token, int])\n",
    "\n",
    "class_names = {'invoice': 0, 'form': 1, 'email': 2, 'handwritten': 3, 'advertisement': 4}\n",
    "NUM_CLASSES = len(class_names)\n",
    "\n",
    "STOP_WORD_S = set(nltk.corpus.stopwords.words('english'))\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset() -> Dataset:\n",
    "    dataset_path = r\".\\dataset\\\\ocr\"\n",
    "\n",
    "    all_files = os.listdir(dataset_path)\n",
    "    doc_ocr_d = {file: content for file, content in tqdm.tqdm(zip(map(lambda f: os.path.splitext(f)[0], all_files), \n",
    "                                                                  map(ocr_input.parse_xml, map(lambda p: os.path.join(dataset_path, p), all_files))),\n",
    "                                                                  total=len(all_files))}\n",
    "\n",
    "    with open(r\".\\dataset\\label.txt\", \"r\") as fp:\n",
    "        label_d = {file: int(label) for file, label in map(lambda line: line.split(','), fp.readlines())}\n",
    "\n",
    "    return [(doc_ocr_d[file], label) for file, label in label_d.items()]\n",
    "\n",
    "# TODO: remove and use predefined splits\n",
    "def split_dataset(dataset: Dataset, test_size: float = 0.2, random_seed: int = random_seed) -> Tuple[List[InputText], List[InputText], List[Label], List[Label]]:\n",
    "    return model_selection.train_test_split(*zip(*dataset), test_size=test_size, random_state=random_seed)\n",
    "    \n",
    "\n",
    "dataset = get_dataset()\n",
    "x_train, x_test, y_train, y_test = split_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))\n",
    "print(len(x_train), len(x_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive vocabulary counting: splitting on space character\n",
    "\n",
    "# Conventions:\n",
    "# index 0 is reserved for unknown tokens that will be mapped to `__UNK__`.\n",
    "# other special token come just after (eg. `__NUM__` for numbers).\n",
    "# other classic token are inserted in order for reverse dictionnary purpose.\n",
    "__UNK__ = '__UNK__'\n",
    "\n",
    "# always put __UNK__ first when redefining special char.\n",
    "DEFAULT_SPECIALS = [__UNK__]\n",
    "\n",
    "def unknown_wrapped(f):\n",
    "    def wrapped(text, vocabulary=None):\n",
    "        gen = f(text)\n",
    "\n",
    "        if vocabulary is None:\n",
    "            yield from gen\n",
    "        else:\n",
    "            for token in gen:\n",
    "                if token not in vocabulary:\n",
    "                    yield __UNK__\n",
    "                else:\n",
    "                    yield token\n",
    "    \n",
    "    return wrapped\n",
    "\n",
    "\n",
    "@unknown_wrapped\n",
    "def basic_tokenizer(text: str) -> Iterator[Token]:\n",
    "    yield from text.split(\" \")\n",
    "\n",
    "\n",
    "def no_preprocess(text: str) -> str:\n",
    "    return text\n",
    "\n",
    "\n",
    "def compute_vocabulary(input_text: List[InputText],\n",
    "                       max_size=1000,\n",
    "                       tokenize_f=basic_tokenizer,\n",
    "                       specials=DEFAULT_SPECIALS,\n",
    "                       preprocess_f=no_preprocess) -> Tuple[Vocabulary, Counter[Token]]:\n",
    "    word_occurences_d = collections.Counter(i for i in itertools.chain(*map(tokenize_f,        # Split into token\n",
    "                                                                            map(preprocess_f,  # Preprocess text before tokenization\n",
    "                                                                                x_train)))\n",
    "                                            if i)                                              # Filter out empty strings\n",
    "\n",
    "\n",
    "    # compute number of missing special tokens in the word occurences\n",
    "    no_missing_special = sum(1 for sp in specials if not sp in word_occurences_d)\n",
    "    vocabulary = collections.OrderedDict([(word, i) for i, (word, _) in enumerate(word_occurences_d.most_common(max_size - no_missing_special), no_missing_special)])\n",
    "\n",
    "    i = 1\n",
    "    for sp in reversed(specials):\n",
    "        if sp not in vocabulary:\n",
    "            vocabulary[sp] = no_missing_special - i\n",
    "            vocabulary.move_to_end(sp, last=False)\n",
    "            i += 1\n",
    "\n",
    "    word_tokenizer_f = partial(tokenize_f, vocabulary=vocabulary)\n",
    "\n",
    "    print(f\"With a vocabulary of size {max_size}, you cover {sum(word_occurences_d[t] for t in vocabulary) / sum(i for i in word_occurences_d.values()) * 100:0.2f}%\")\n",
    "\n",
    "    return vocabulary, word_tokenizer_f, word_occurences_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vocabulary(vocabulary, n=1000):\n",
    "    plt.plot(list(range(n)), [i for _, i in vocabulary.most_common(n)])\n",
    "\n",
    "    plt.title(f\"Évolution du nombre d'occurrence des {n} tokens les plus fréquents\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_accumulated_vocabulary(vocabulary, n=1000):\n",
    "    total_tokens = sum(vocabulary.values()) / 100\n",
    "    plt.plot(list(range(n)), list(itertools.accumulate(i / total_tokens for _, i in vocabulary.most_common(n))))\n",
    "\n",
    "    plt.title(f\"Évolution du nombre d'occurrences cumulé des {n} tokens les plus fréquents rapporté au nombre total de tokens\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary, _, word_count = compute_vocabulary(x_train, max_size=10**4)\n",
    "plot_vocabulary(word_count, n=10000)\n",
    "plot_accumulated_vocabulary(word_count, n=10000)\n",
    "print(len(word_count))\n",
    "\n",
    "print(list(vocabulary.items())[101:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We must reduce vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_preprocess(text: str) -> str:\n",
    "    text = re.sub(r\"([.?,!:])\", r\" \\1 \", text.lower())  # Add an extra space before punctuation (usefull in english)\n",
    "    return re.sub(r\"[^ a-zA-Z0-9.?,!:$£€@#/\\-\\+\\\\\\*]\", \" \", text)  # Remove any non basic character\n",
    "\n",
    "\n",
    "@unknown_wrapped\n",
    "def stop_word_tokenizer(text: str) -> Iterator[Token]:\n",
    "    for word in text.split(\" \"):\n",
    "        if not word in STOP_WORD_S:\n",
    "            yield word\n",
    "\n",
    "ALPHA = {i for i in \"azertyuiopqsdfghjklmwxcvbnAZERTYUIOPQSDFGHJKLMWXCVBN\"}\n",
    "DIGIT = {i for i in \"1234567890\"}\n",
    "PUNCT = {i for i in r\".?,!:$£€@#/-+\\*\"}\n",
    "\n",
    "\n",
    "__NUM__ = \"__NUM__\"\n",
    "__PUN__ = \"__PUN__\"\n",
    "__MIX__ = \"__MIX__\"\n",
    "MY_SPECIALS = [__UNK__, __NUM__, __PUN__, __MIX__]\n",
    "\n",
    "\n",
    "@unknown_wrapped\n",
    "def special_tokenizer(text: str) -> Iterator[Token]:\n",
    "    for word in text.split(\" \"):\n",
    "        if not word in STOP_WORD_S:\n",
    "            if all(c in ALPHA for c in word):\n",
    "                yield word\n",
    "            elif all(c in DIGIT for c in word):\n",
    "                yield __NUM__\n",
    "            elif all(c in PUNCT for c in word):\n",
    "                yield __PUN__\n",
    "            else:\n",
    "                yield __MIX__\n",
    "\n",
    "\n",
    "vocabulary, _, word_count = compute_vocabulary(x_train, max_size=10**2, tokenize_f=special_tokenizer, preprocess_f=regex_preprocess, specials=MY_SPECIALS)\n",
    "plot_vocabulary(word_count, n=10000)\n",
    "plot_accumulated_vocabulary(word_count, n=10000)\n",
    "print(len(word_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Model: Bag of Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = 10**3\n",
    "\n",
    "vocabulary, tokenizer_f, _ = compute_vocabulary(x_train, max_size=VOCABULARY_SIZE, tokenize_f=special_tokenizer, preprocess_f=regex_preprocess, specials=MY_SPECIALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocabulary)\n",
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer(vocabulary=vocabulary, tokenizer=tokenizer_f, preprocessor=regex_preprocess, analyzer=\"word\")\n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "print(regex_preprocess(x_train[0]))\n",
    "print(vectorizer.fit_transform(x_train[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(input_dim=VOCABULARY_SIZE, units=128, activation=\"relu\"),\n",
    "    keras.layers.Dense(units=1024, activation=\"tanh\"),\n",
    "    keras.layers.Dense(units=1024, activation=\"tanh\"),\n",
    "    keras.layers.Dense(units=NUM_CLASSES, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"sparse_categorical_accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(vectorizer.fit_transform(x_train[:-100]), y_train[:-100], epochs=30, batch_size=64, validation_data=(vectorizer.fit_transform(x_train[-100:]), y_train[-100:]), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(vectorizer.fit_transform(x_test), y_test, verbose=2)\n",
    "\n",
    "y_pred = model.predict_classes(vectorizer.fit_transform(x_test))\n",
    "\n",
    "print(class_names.keys())\n",
    "print(sklearn.metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bit more complex: Recurrent Neural Networks and Long-Short Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = 10**3\n",
    "EMBEDDING_SIZE = 64\n",
    "MAX_SEQ_LEN = 10**3\n",
    "\n",
    "# vocabulary, tokenizer_f, _ = compute_vocabulary(x_train, max_size=VOCABULARY_SIZE, tokenize_f=special_tokenizer, preprocess_f=regex_preprocess, specials=MY_SPECIALS)\n",
    "vectorizer = lambda text, vocabulary=vocabulary: map(lambda token, vocabulary=vocabulary: vocabulary[token], special_tokenizer(text, vocabulary))\n",
    "print(*vectorizer(x_test[0]), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(VOCABULARY_SIZE, EMBEDDING_SIZE, input_length=MAX_SEQ_LEN),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(128)),\n",
    "    keras.layers.Dense(units=NUM_CLASSES, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"sparse_categorical_accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(vectorizer.fit_transform(x_train[:-100]), y_train[:-100], epochs=30, batch_size=64, validation_data=(vectorizer.fit_transform(x_train[-100:]), y_train[-100:]), verbose=2)"
   ]
  }
 ]
}