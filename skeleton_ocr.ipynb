{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "colab": {
      "name": "skeleton_ocr.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clemsage/NeuralDocumentClassification/blob/master/skeleton_ocr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56cA2WtUQZ-I",
        "colab_type": "text"
      },
      "source": [
        "# Training a classifier on OCR text input\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAd75X5JT4vI",
        "colab_type": "text"
      },
      "source": [
        "## Imports & Cloning repository\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uefjUmcMRnLT",
        "colab_type": "text"
      },
      "source": [
        "### Import Tensorflow v2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVIlDgNHW62j",
        "colab_type": "code",
        "outputId": "affbf537-aad1-4c1f-f2d3-69c1885b6c40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install tensorflow-gpu==2.0\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/44/47f0722aea081697143fbcf5d2aa60d1aee4aaacb5869aee2b568974777b/tensorflow_gpu-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (380.8MB)\n",
            "\u001b[K     |████████████████████████████████| 380.8MB 43kB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (3.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (0.1.8)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/9e/a48cd34dd7b672ffc227b566f7d16d63c62c58b542d54efa45848c395dd4/tensorboard-2.0.1-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 59.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (0.8.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.0.8)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 52.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.11.2)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (0.2.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (0.8.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.17.4)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (3.10.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (0.33.6)\n",
            "Collecting google-auth<2,>=1.6.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/cb/786dc53d93494784935a62947643b48250b84a882474e714f9af5e1a1928/google_auth-1.7.1-py2.py3-none-any.whl (74kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 12.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (0.16.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (41.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (0.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0) (2.8.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (0.2.7)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (3.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (0.4.7)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (3.1.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (2.21.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (2.8)\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 2.0.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 2.0.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorboard 2.0.1 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.7.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: google-auth, tensorboard, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: google-auth 1.4.2\n",
            "    Uninstalling google-auth-1.4.2:\n",
            "      Successfully uninstalled google-auth-1.4.2\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "Successfully installed google-auth-1.7.1 tensorboard-2.0.1 tensorflow-estimator-2.0.1 tensorflow-gpu-2.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ak-qq01rXLxL",
        "colab_type": "code",
        "outputId": "e13d90dd-f564-4edc-f336-708563b00d6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Check your devices, if it fails change your execution context to GPU\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7Ki-ZY9RyXl",
        "colab_type": "text"
      },
      "source": [
        "### Usefull imports and clone repo\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vFz745yYfbc",
        "colab_type": "code",
        "outputId": "170243bc-d33c-4199-de56-571cdda914de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "print(sys.version)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.6.8 (default, Oct  7 2019, 12:59:55) \n",
            "[GCC 8.3.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HL_khQsbYY7O",
        "colab_type": "code",
        "outputId": "64706bd2-864b-4f6d-ad17-3834c0df8463",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Clone the git repository\n",
        "\n",
        "if not os.path.exists('NeuralDocumentClassification'):\n",
        "  !git clone https://github.com/clemsage/NeuralDocumentClassification.git\n",
        "else:\n",
        "  !git -C NeuralDocumentClassification pull\n",
        "sys.path.append('NeuralDocumentClassification')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'NeuralDocumentClassification'...\n",
            "remote: Enumerating objects: 140, done.\u001b[K\n",
            "remote: Counting objects: 100% (140/140), done.\u001b[K\n",
            "remote: Compressing objects: 100% (110/110), done.\u001b[K\n",
            "remote: Total 140 (delta 70), reused 78 (delta 28), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (140/140), 307.29 KiB | 592.00 KiB/s, done.\n",
            "Resolving deltas: 100% (70/70), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lp4eXRIKW62o",
        "colab_type": "code",
        "outputId": "42496dde-edfd-4171-ced0-772a0823c03e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Lot of usefull imports\n",
        "\n",
        "# All of them are already installed on the colab session\n",
        "\n",
        "\n",
        "# STD imports\n",
        "import collections  # contains idiomatic data structures\n",
        "import itertools    # provides efficient tools on iterators\n",
        "import re           # regexes\n",
        "\n",
        "from functools import partial  # little helper for partially applying a function\n",
        "from typing import List, Dict, Tuple, Union, NewType, TypeVar, Counter, Iterator  # statically typing for python\n",
        "\n",
        "import matplotlib.pyplot as plt  # plotting tool\n",
        "import nltk                      # natural language processing toolkit\n",
        "import numpy as np               # main scientific linear algebra library in python (matrices)\n",
        "import pandas as pd              # dataframes\n",
        "import sklearn                   # machine learning & data mining library\n",
        "import tqdm                      # progression bar\n",
        "\n",
        "from tensorflow import keras     # high level tensorflow API\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 9)\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNr1m0q_Tjo9",
        "colab_type": "text"
      },
      "source": [
        "### Defining some constants and types"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WheOgS6TW62r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Some usefull types for this dataset\n",
        "\n",
        "InputText = NewType('InputText', Union[str, List[str]])\n",
        "Label = NewType('Label', int)\n",
        "DocumentRecord = NewType('DocumentRecord', Tuple[InputText, Label])\n",
        "Dataset = NewType('Dataset', Dict[str, List[DocumentRecord]])\n",
        "\n",
        "Token = NewType('Token', str)\n",
        "Vocabulary = NewType('Vocabulary', Dict[Token, int])\n",
        "\n",
        "\n",
        "# Constants\n",
        "\n",
        "CLASS_NAMES = ['form', 'email', 'handwritten', 'advertisement', 'invoice']\n",
        "CLASS_INDICES = ['1', '2', '3', '4', '11']\n",
        "NUM_CLASSES = len(CLASS_NAMES)\n",
        "\n",
        "STOP_WORD_S = set(nltk.corpus.stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YD9QVg8KW62v",
        "colab_type": "text"
      },
      "source": [
        "## Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeZWsk-AbTc_",
        "colab_type": "code",
        "outputId": "8900867d-7de6-42bd-c18b-a9e4d02de3bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Some local scripts imports\n",
        "import download_dataset  # dowloading from google drive\n",
        "import ocr_input         # deals with reading dataset and xml parsing\n",
        "\n",
        "\n",
        "for elt in ['label', 'ocr', 'dataset_assignment']:\n",
        "  download_dataset.download_and_extract(elt)\n",
        "dataset_path = 'dataset'"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading ./dataset/label.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "194kB [00:00, 38.8MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading ./tmp/ocr.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "25.5MB [00:00, 35.0MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Unzipping ./tmp/ocr.zip to dataset/ocr…\n",
            "Downloading ./dataset/dataset_assignment.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "289kB [00:00, 45.8MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngR65gEjW62w",
        "colab_type": "code",
        "outputId": "e6b29807-d0b5-46e2-c19c-df370726279a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "def get_dataset() -> Dataset:\n",
        "  \"\"\"\n",
        "  Parse all data xml files and make pairs withe their label.\n",
        "  Splits the records into training and test datasets.\n",
        "  \"\"\"\n",
        "\n",
        "  # Parsing xml files into doc_ocr_d\n",
        "  all_files = os.listdir(os.path.join(dataset_path, \"ocr\"))\n",
        "  doc_ocr_d = {file: content for file, content in tqdm.tqdm(zip(map(lambda f: os.path.splitext(f)[0], all_files), \n",
        "                                                                map(ocr_input.parse_xml, map(lambda p: os.path.join(dataset_path, \"ocr\", p), all_files))),\n",
        "                                                            total=len(all_files))}\n",
        "  \n",
        "  # Fetching labels into label_d\n",
        "  with open(os.path.join(dataset_path, \"label.txt\"), \"r\") as fp:\n",
        "      label_d = {file: CLASS_INDICES.index(label.strip()) for file, label in map(lambda line: line.split(','), fp.readlines())}\n",
        "\n",
        "  # Fetching assignments into dataset_splits\n",
        "  dataset_split = {\"training\": [], \"test\": []}\n",
        "  with open(os.path.join(dataset_path, 'dataset_assignment.txt'), 'r') as f:\n",
        "    for line in f.readlines():\n",
        "      line = line.strip()\n",
        "      file_id, assignment = line.split(',')\n",
        "      dataset_split[assignment].append(file_id)\n",
        "\n",
        "  return {split_name: [(doc_ocr_d[file], label_d[file]) for file in file_split_l] for split_name, file_split_l in dataset_split.items()}\n",
        "\n",
        "dataset = get_dataset()\n",
        "\n",
        "print(f\"Number of training documents: {len(dataset['training'])}\")\n",
        "print(f\"Number of test documents: {len(dataset['test'])}\")\n",
        "\n",
        "x_train, y_train = zip(*dataset[\"training\"])\n",
        "x_test, y_test = zip(*dataset[\"test\"])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 16275/16275 [00:02<00:00, 6456.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of training documents: 12952\n",
            "Number of test documents: 3323\n",
            "LLZS TSLLS cz ~>i®o-z~   2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYPlOzpCW622",
        "colab_type": "text"
      },
      "source": [
        "## Study the vocabulary\n",
        "\n",
        "In this part we will look at the data.\n",
        "\n",
        "When dealing with text and words, the first thing to do is looking at those words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW93AiRT8b4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To access a specific element or range in a list, you can use bracket notation: \n",
        "# `my_list[0]` is the first element\n",
        "# `my_list[10: 20]` is an array containing elements from index 10 (included) to 20 (excluded)\n",
        "\n",
        "## Print some texts from the dataset and look at what the OCR system has read. ##\n",
        "\n",
        "\n",
        "## Any remarks ? ##"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gha_z0QP0EHJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "cellView": "form",
        "outputId": "d36e03c2-9446-47e0-b7d8-d64bcd1c8293"
      },
      "source": [
        "#@title\n",
        "\n",
        "# Use the function `print` to look at texts in the datasets (either x_train or x_test)\n",
        "# To access a scpecific element or range in a list, you can use bracket notation: \n",
        "# `my_list[0]` is the first element\n",
        "# `my_list[10: 20]` is an array containing elements from index 10 (included) to 20 (excluded)\n",
        "\n",
        "\n",
        "# print some texts from the dataset and look at what the OCR system has read.\n",
        "for i in range(10):\n",
        "  print(i, x_train[i])\n",
        "\n",
        "# Any remarks ?\n",
        "\n",
        "\"\"\"\n",
        "Mostly not words, bunch of symbols. Very hard to understand.\n",
        "\"\"\""
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 -4~&4 9~j~ 6~ 6/ 90 ~a 33 39 /o 3i-3y 3b yl vo - yo 30 3~6 t 6y 59 60 70 A~, al-34 95 a~ 35 XAuE CoLLEGE -t- 91 18 4e 39 6/ aW 51440 9402 67 3.-A J\t56\t-7S 36\tyy\t30 61\tS7 70 qa 0 6/ cia ii I6 7 /a /o 1a~~ 9, . aa as iy aS a~ r-L, ~57 S9 63 -5 6 5 / Sa CR)Lp IN 014 50 .50 iib 5~ous~ poi 6MaKE2 a9 30 30 s4 .57 .5a ~9 .59 Sy S 1 .5 / y 7 ,~0 3a y9 T /8 2 94A.~& 64-XL !a a6 76 9v~ &-~ N~ ~`/~ 3'V NH ~~ut~ + 5 a 73 -7'7 73 Ea 3 3 48 ag as 16 o-: L, a 701- /a 7 ad a5 a8 a9 ay a9 ay -a3 aa a6 a3 7t 77 7 s 7y 77 7 ~5 U ~J ?~., [iiLi ~ a 7 3.~ iy y`3 a 3 y3 01 -Yo 571 39 a6 3y 311 ay 37 yo 3a LLJ  ~ yy ~ 4/ 5o S3 .5y 6a ta59 ~ ~&I ~~ {_ F 4,;a '7D.t- 51440 9404 7ot yo - ,&~ -~7 o* 5y* ~9-1` Sy~` ~~~ &Ail ~ /3 /y 13 13 13 /a l ~ .zj~ )t q /3 r 3~~ ~ ~ / y 13 7 /y 9 / a 13 / 0-2 /y 13 7 9 6 8 3 7 6 9 P ~:.~ ~ ~ ~~I _ ~. wo~i~~/ ~ ao /7 a~ /8 ;~dj- aI -3~ /7 /71 0 32- 41 a94` 30~` iC/ aG 31 a- /6 a3 ay - G /9 /7 /s i ~ 13 /a /~) 4 -4~- ~a-~ 4riX-1'4 A-c~ ' ' L T -Afii~.' ,..~. y3 S/ 57 y 9 y~2 S Al S S y4 At' a(-3q 35 33 32 33 33 34 3s 35+ ~5 67 bS ~ 7 l07 16 `/ los /W a) - 3 q AA 3115 + /7 /7 /-t,) 17 /1 31 34 34 J3 35 r a--3~f 17 I.S 17 , r 3~+ 3S y~ 3a y~ 3 ,-~~iuE CoLLSGE t- C}+i L D i IJ }f' ~f 5a q g -,~& 5~c~sE ): 67 snAc~E9 )9 d q ao ~~ -yg .6a ao 80 44-A 51440 9405 70t 30 a9 31 70 iiI1 / i7 n i7 O i7 04/ 3/ 97 33 a3 3'1- 16 / 7 /3 /y 37 [=S-9 ~~ s5 ~ S ~~ ! l 6.~ 5 9 a6~ s7 0 sa a 55 0 5a. C\"/~' ~~ 35 3a a1 3 q  3.~ a7 3y!' i L r -4rx.&t\" fa~~ #,#/ 4t 4 yo- i ~ Ll~ ! I `/ -Z&t 51440 9406 _ ~a 7ot ~ a ~o - a2d\" id a1 /7 17 i5 tI-2J 910 4e1a##/ ay aa ag a6 ~4ez4w ,40j,02~ ad as 3) ~(~ 41&me /0 9 IF S /Q 7 /o Atd.llLtJ,6,u 33 a9 3a 31 3q 67 7/ 68 69 ~~ 7G -73 9'IaZ\"~u,~~ae1~ a y a 7 33 7 M7~ a5 s5 ~6 3~ / a 5 io ~ 33 a7 30 67 73 70 iEEii/3 76 G8 76 6H 76 aG ab a3 ay ~3/ as 36 ay L2J yw &,m9L1.~ y yo Y8 36 ya yl yo 37 y0 y8 y/ 39 !f/f /\" 6w~ ya 4/ 3 yy yl 33 yZ Y5 yo 37 d0 yl ,qo~ , 1ij 7y 7Y 77 L T . 5I440 9407 7ot yo - 7,01- yo - y-,o` v 9~` Sy ~.uu~o L,.~J -   SS~ F35:f ~-~°~\"~ ly /6 9 /6 13 /S Iy l8 ~y 17 iS >6  ~~~ /p 6 CL9) 7 /7 ~~--- 13 ia ib s 13 13 13 C 3q 36* 3a.* a9~ 30~ ~~ 6uv ## f aa 13 ao ~o aa l9 ~9 ~S j 7 ~9 ao 5~ 9naip, a) ~. 9'~ a/, -6 ~ ~I~;~.« (9 ou ql&t  \n",
            "1 LLZS TSLLS cz ~>i®o-z~  \n",
            "2 ~Q4 ` Are- Z7/a,:2- L,3,fl,ff lei Il~ tam G; d     ~ ~  ~ ~ /~ ~~ ~~~3 ~~ y U ~107 J. ~ \n",
            "3 ]1Ja IL~ ~'~~* L~  3~_ !fI.: - . , FF,/tit A,t~ / fs - Z° IZ-f= U- iym ~ S~, Q (1 N~-S ~- ~~ r ra~~_. xc- Lu,~--~-~ I~ ,. ~ ~ ~1'~- ~ Gc~ c a~L ?  l%yL~`. ~. ~~O G~G!/~L;/J rh(- /np.~l~L~ . ,~L! ce...l ~~I~lu Sfrc E m - X, 'C~ ~- Ajl F;-4 ~ , f i - --- r  . ,. ..t /E\" 20 (c .21- -'/ ( [y ~ y!,~ ~((r'~c -: J rz-  Ji , ~< <_ ;~tZ ^ \n",
            "4 Juab Middle School 475 East 600 North Nephi, UT 84548 May 7, 1996 Camei ciggerettes R.O. Box 7 Wi nstort sal em, NC 67102 Dear Sir/I°Iedam, You should stop selling ciggarettes because smoking kills. Everlj User 390,000 people die because of ciggarette-reiated deaths. You should stop selling ciggarettes because it can cause heart disease. About 150,000 people die of ciggarette-related heart attacks, You should also stop selling ciggarettes because ciggarette smoke contains toxins. The smoke contains: Cyanide (rat poison), Formaldehyde (a chemical used to preserve the dead), Methanol (wood alcohol), Acetylene (fuel for blow torches), Arnmonia (a cleaning chernical), and acetone (a chemical in finger polish remover). The smoke also contains the deadly gases Nitrogen Oxide and Carbon Monoxide. You should also stop using \"Joe\" the camel cartoon character because he influences younger people to start smoking. Every day more than 3,000 teens start smoking.Thats more then 1,000,000 every year, You should stop putting nitocine 1n your ciggarettes because nitocine is an addictive poison. Taken in large amounts nicotine paralizee breathing muscles. Taking an amount as large as 1/5 of an asprin tablet can be as deadly as cyanide. si,ncerelg, WefvA~ Jess Worwood Juab Middle School 475 E. 800 North Nephi, UT 84648 Camel P.O. Box 7 Winston Salem, NC 27102 Dear Sir/Madam: I would like to tell you about how your cigarettes are killing people who use them and people who don't use them. How cigarettes kill people who use them are they can get lung cancer and they may have heart problems. Cigarettes can also make the smoker very ugly on the inside and on the outside. It makes the smoker's fingers turn yellow and their teeth are not white any longer and the turn yellow and their breath is not a fresh smell. Cigarettes can also kill the people who do not smoke. People who don't smoke can get lung cancer from second hand smoke. Second hand smoke is almost as bad as regular smoking. All the cigarettes around the world are killing and putting people's lives in danger. Millions and millions of people are dieing every year because of the damage the cigarettes do to their lungs and heart. Smoking can also make it harder to breath. Women who smoke while they are pregnant are putting themselves and the child in serious danger. Smoking can cause the baby to be disabled or can cause the baby to have serious diseases or have very bad head injures. If you would think hard about not putting out so many cigarette advertisements out so that kids will not use them to get away from their problems at home or at school. We maybe could save some lives from getting killed and hurt in every way. Sincerely, Q,naoc(a ~~wlatw(' ~ N -P N Amanda Sutherland --~ V A W Juab Middle School 475 East 800 North hlephi, UT 84648 April 22, 1996 Camel P.O. boa 7 Vflnston Salem, NC 27102 Dear Sirr'MSdam: I am writing this letter to let you know how I feel about your product. Your Camel r,igara,i,tF,s are killing rrlany people, arrd you know it. But you still try to deny it! 'I-ir,'w r.'.ouW y[+ii l iiie s.rii.-ii gjr.lrtelf, knowing thttt you 8re killing so many people? `It~u might ~U)y'\"rjh, the.y'rt; the ones making the choice to smoke,\" but they can't smoke If there isn't a cigarette to srnoke. I know a lady who Is a secretary for my father, and she smokes every chance she gets. She will die pretty soon because of ernphyserna Emphysema is caused by s'rr,otcing, and it's.not the only thing smoking causes. Smoking slows you down, takes you away from your job, limits your abilities and much more. I cr,uld go on and on about what smoking does for you, but why don't you just face it, smoking is bad for you. My grandfather also srnokes He has tried to quit many tirries, but can't. It is because he Is so addicted to smoking. I would like my grandfather to live as ling as he possibly can, and also have a happy life and be healthy. f;ut because of your product these things aren't possibie I am a student at Juab Middle School and I know for a fact that there are rrany;y kids here that srnoke, When kids smoke they are choosing what they want their future to be like. They must want a terrible future, because they are just throwing their lives down the drain. Why don't we try to help the people in krnerira, not kill them';i Sircerely, LeAnn Hall CGn~ PA (~.D, PvoK=1 ;ca.t e n1 eJ G a91 c~ u,)', nf3c cNn D . ear Sir 1f'IadaiYle .5ucub MId(81-p- ` ~C;1Aool /Aq5FC4* goo C)o~+l. MPph, g4rtnWB n1aaLO I hls lost Liuater in health we have been learning about druga and Alcohol As a rtlerflC+er of a p12C11iC Sci-IOol I can see how drugs can affect the kids around rrie.iarn writing because I d7n't think that it's fair to kids rny age and younger to die at an early age just because of smoking.l know that they might have chose to try their first ciggerate but puting thern in places were kids can get to theni. When f was little and even now I have had at least on reletive go to jail because of stealing drugs or alcohol products.Rlght now I have an uncle ;vi-ro is in juviniai for steaiing,saie.ing drugs and sniffing.And it all started because of one ciggaret. wiiy are you selling things that you know can harm people do you enjioy wacthing people die?1 know that this letter won't stop you from selling u.our product but please consider my plead. 7 Si ric,,erly Kayl a greenghalh J~~w'ySC_14A_Y_~1TGS ---------- , ---------! ~--~ -~O-Y~~l_--~~'_l °-------------------------- _____.-_-- ---------------- \t \t \t -------- -\t' ------------ ~ jA - - i\t~ -~D qthp-~Z1LL~~_ _Q.l~'~~-- ~ ~~ _wl !kA n Abrzml&y-oc--,Zm-cn-6 \t--_-- _ -- : - ~\t --- - -~-----_-` - ~~~ ~.~V-~ ~~ ~- - - -  ~_ -_~\t__- ------ --l=~aAk A - - --~----------- ------1---- ; -----i-._-_ ---I-- ~ -----~-- ----- w W , ~ ~ :\tNcp*~ _ ~ ~,ca.~`~~~ .__------- ---\t_ ----------- -___ ----------------------- ---.Jl zllu_. ----- - - --------- 1~-- - AMcL n ~ uuV- p c~d~ef_~°~`~u~ - ~~ - ----- - -~. ~y_ ~--  _ _~ _-------------------------- \t~- r ~r _ ----\t ~ u~~-111I p~t--~u,l ~ ~ 4 ~1~ ~-~-- ----- -------\t~ ~ ~ , , - }- _ ~11~=zi-f~l~~/ ~~ _j l! _~'1~( \"- C 0,.~~. ~11~_V_~.~~~3; _ !r ~11Y~! \tL~ s`_~~~ 1~  \t~ - -' ~ ouv  - --\tI ---- ~ ~  ------\t~~ V~+C1 t ~y'C r(Aj~ 'n~ 1 -t at`- f- -- r- - - - - ' - -- ~ -- ~ \t~ . ,\t--  ----'---\t ----------- ____ -------'-' . - . - -ry ----'---.~ -----i'--'---: ---------..------------------'--- Ln   - -- r ~~  1 . - --~Y_Z-~Gt~-~---'~ce ~ .. ~~,,... '~ `~._____ ...------------- \n",
            "5 \n",
            "6 ~~ 1144 ollk, Os/lboft ( r~ \"~ /9 9 ~' \n",
            "7 `!1p 01I .-~1 1 [e_C_)_I_ I, ' - ~O To ~~ howr~ G wcc~~ or~ _b~ 1_.Ye_c~%,ic_v ~ab_ I>_--T` 5c1.~ . ~}- -1.-hi_S. .~ ~ . C` St r1~~ ,)(/ U LA C 1 6_I11! Yn -- At,A~d.V-4 0ooks_..._.._-.___ (J 1~ _.. 4 S~. C7rt.G_cl.~_G.~.1.~ `j ~9__., -- -- GoY+~i.n.. ~--- ~ _ Ih.~; _ ~----- r<' ' QI-C -b Lrr.. __~J ~--- e--) ---- --- -- - -_~ R'T ~J ~ !. \n",
            "8 W 7~ pm ---- ~ ti 1 I Glun 1~wiJ/Y A S SS S S~ ~`Gd ~k t J M J/G'd vs: 7 .23 y ky 5' 3f: 3 2~3 ~-*~0 ±.~a oP NOV- ~ ki N1. C~ ~~~v~ \n",
            "9 ZC?\\ LZ ~ca bZ X~'~, CJ ~  CV~~ QHS3uI WId 36f6Z!fsQ r_t~ : T? Gosc Gztias ~~ ~AwU1Q~6 b~ 1 L vvbu~2g,~q C~ (o  Mk-~ oay2 ~ o M1 i Pw~~~~2-Ifl o~ -:K'm k~1oM \"-w\"o-06 Col ~(y \\N~UtA,gr Uc.~~\\ &~ Q~~ 0 o~ -\\C, M U)O~,3ms(-, .~ cw)-~ o\\g1. rt\" wM OOQ~00 ooCU ~AOvc ~02WC rO vmUaP3 nod~ \\ M(~ on NU21ih - Qb~'m a\\UBvLb CY-) LM io Csn~\" - ~ ~ACJ~ CS)fl, u~ a\\~Q o1c~9~ o~r~~ L~J~) 03   C9~~(~ a~~V-q ~lm\\m \\A -k0b Mi11ian b e~ D` Vn L~Q~r 'ro Cap, T`OJ1'll~o C!OW Wi11 Cpv ~J dC~> (0`-) (:)Ur Co-vnvrrvni~q ~ObGcta 6nGQ w)g alf) .Qx-~~ lk9t~mo~P upS 01Q~) Wo \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmATmRuC-OT4",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAtqeISKW623",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Naive vocabulary counting: splitting on space character\n",
        "\n",
        "# Conventions:\n",
        "# index 0 is reserved for unknown tokens that will be mapped to `__UNK__`.\n",
        "# other special token come just after (eg. `__NUM__` for numbers).\n",
        "# other classic token are inserted in order for reverse dictionnary purpose.\n",
        "__UNK__ = '__UNK__'\n",
        "\n",
        "# always put __UNK__ first when redefining special char.\n",
        "DEFAULT_SPECIALS = [__UNK__]\n",
        "\n",
        "def unknown_wrapped(f):\n",
        "    \"\"\"\n",
        "    A wrapper around a tokenizer that provides a vocabulary parameter.\n",
        "    If vocabulary is not None, generated token are checked against the vocabulary.\n",
        "    If it does not contains this specific token, __UNK__ is yielded instead.\n",
        "    \"\"\"\n",
        "    def wrapped(text, vocabulary=None):\n",
        "        gen = f(text)\n",
        "        if vocabulary is None:\n",
        "            yield from gen\n",
        "        else:\n",
        "            for token in gen:\n",
        "                if token not in vocabulary:\n",
        "                    yield __UNK__\n",
        "                else:\n",
        "                    yield token\n",
        "\n",
        "    return wrapped\n",
        "\n",
        "# the most basic tokenizer: split on space charater\n",
        "@unknown_wrapped\n",
        "def basic_tokenizer(text: str) -> Iterator[Token]:\n",
        "    yield from text.split(\" \")\n",
        "\n",
        "\n",
        "# the most basic preprocess: no preprocess done\n",
        "def no_preprocess(text: str) -> str:\n",
        "    return text\n",
        "\n",
        "\n",
        "def compute_vocabulary(input_text: List[InputText],\n",
        "                       max_size=1000,\n",
        "                       tokenize_f=basic_tokenizer,\n",
        "                       specials=DEFAULT_SPECIALS,\n",
        "                       preprocess_f=no_preprocess) -> Tuple[Vocabulary, Counter[Token]]:\n",
        "    \"\"\"\n",
        "    Given a preprocessing function, a tokenizer and a collection of special tokens,\n",
        "    compute the vocabulary mapping and a corresponding tokenizer and number of occurences of tokens.\n",
        "\n",
        "    main steps:\n",
        "      - First preprocessing is applied to each text.\n",
        "      - Then each preprocessed text is splitted into tokens.\n",
        "      - All tokens from all text are chained together and empty tokens are filtered out.\n",
        "      - Tokens are sorted by reversed number of occurences in the vocabulary.\n",
        "      - A special treatment is reverved for special tokens.\n",
        "\n",
        "    return:\n",
        "      - vocabulary: A mapping from tokens to their corresponding index. Indices start at 0 and end at max_size-1\n",
        "      - word_tokenizer_f: A tokenizer function that only produce tokens included in the vocabulary. (__UNK__ is returned if the token is not in the vocabulary)\n",
        "      - token_occurences_d: A mapping from tokens to their corresponding number of occurences in the texts.\n",
        "    \"\"\"\n",
        "    token_occurences_d = collections.Counter(i for i in itertools.chain(*map(tokenize_f,        # Split into token\n",
        "                                                                            map(preprocess_f,  # Preprocess text before tokenization\n",
        "                                                                                x_train)))\n",
        "                                            if i)                                              # Filter out empty strings\n",
        "\n",
        "\n",
        "    # compute number of missing special tokens in the word occurences\n",
        "    no_missing_special = sum(1 for sp in specials if not sp in token_occurences_d)\n",
        "    vocabulary = collections.OrderedDict([(word, i) for i, (word, _) in enumerate(token_occurences_d.most_common(max_size - no_missing_special), no_missing_special)])\n",
        "\n",
        "    # Put special tokens at the beginning of the vocabulary\n",
        "    i = 1\n",
        "    for sp in reversed(specials):\n",
        "        if sp not in vocabulary:\n",
        "            vocabulary[sp] = no_missing_special - i\n",
        "            vocabulary.move_to_end(sp, last=False)\n",
        "            i += 1\n",
        "\n",
        "    # Specialize the given tokenizer for the computed vocabulary\n",
        "    word_tokenizer_f = partial(tokenize_f, vocabulary=vocabulary)\n",
        "\n",
        "    return vocabulary, word_tokenizer_f, token_occurences_d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0n5i6xkXW625",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Some plotting functions to display the vocabulary\n",
        "\n",
        "def plot_token_count(token_count, n=1000):\n",
        "    \"\"\"\n",
        "    Plots occurences for n most common tokens\n",
        "    \"\"\"\n",
        "    plt.plot(list(range(n)), [i for _, i in token_count.most_common(n)])\n",
        "\n",
        "    plt.title(f\"Evolution of occurences of the {n} most frequent tokens\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_accumulated_token_count(token_count, n=1000):\n",
        "    \"\"\"\n",
        "    Plots accumulated occurences divided by token number of tokens for n most common tokens \n",
        "    \"\"\"\n",
        "    total_tokens = sum(token_count.values()) / 100\n",
        "    plt.plot(list(range(n)), list(itertools.accumulate(i / total_tokens for _, i in token_count.most_common(n))))\n",
        "\n",
        "    plt.title(f\"Evolution of cumulated occurences of the {n} most frequent tokens divided by total number of tokens\")\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsIOx-1YIs-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Use the function `compute_vocabulary` to get vocabulary and token_count object. ##\n",
        "\n",
        "## What are the most common tokens ? ##\n",
        "\n",
        "## Plot token occurences and cumulated token occurences. ##\n",
        "\n",
        "## How many percentages of the token are included in the vocabulary if we use 1_000 tokens ? 10_000 ? 100_000 ? ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uiZNl1mW628",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "# Use the function `compute_vocabulary` to get vocabulary and token_count object.\n",
        "*_, word_count = compute_vocabulary(x_train, max_size=10**4)\n",
        "\n",
        "\n",
        "# What are the most common tokens\n",
        "print(list(word_count.most_common(100)))\n",
        "\n",
        "# Plot token occurences and cumulated token occurences.\n",
        "plot_token_count(word_count, n=10000)\n",
        "plot_accumulated_token_count(word_count, n=10000)\n",
        "\n",
        "# How many percentages of the token are included in the vocabulary if we use 1_000 tokens ? 10_000 ? 100_000 ?\n",
        "for size in [10**3, 10**4, 10**5]:\n",
        "  print(f\"With a vocabulary of size {size}, you cover {sum(t for _, t in word_count.most_common(size)) / sum(word_count.values()) * 100:0.2f}% of the encountered tokens\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKGIAJU1W62_",
        "colab_type": "text"
      },
      "source": [
        "## We must reduce vocabulary size\n",
        "\n",
        "To clean the texts from all the noise produced by the OCR, we can use advanced preprocessing and tokenizer.\n",
        "\n",
        "The job of the **preprocessing** is to prepare the text to be splitted on space characters. An example of simple preprocessing would be:\n",
        "* Use lowercase only.\n",
        "* Remove useless characters that are unlikely to really be in the document and likely to be noise produced by OCR.\n",
        "* Introduce additional spaces between words and punctuation so \"This is a cat.\" is transformed into \"This is a cat .\" (note the space at the end).\n",
        "\n",
        "The job of the **tokenizer** is to split sentences into separate tokens. Our vocabulary is polluted by multiple punctuation and numbers. A simple workaround is to create special tokens that represent a group of symbols. For example we could introduce a `__NUM__` token wich represent all numbers. Any number in the text would be mapped to `__NUM__`.\n",
        "\n",
        "The resulting vocabulary should include much less noise and a lot a words !\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsDPsGe8P5jV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Implement the preprocessor described above. Feel free to add other steps in the preprocessing. ##\n",
        "\n",
        "def my_preprocess(text: str) -> str:\n",
        "  # Implement here\n",
        "  return text\n",
        "\n",
        "## Implement the tokenizer described above. Examples of groups of tokens are: numbers, punctuation, mix of those... ##\n",
        "\n",
        "# Some categories of character\n",
        "ALPHA = {i for i in \"azertyuiopqsdfghjklmwxcvbn\"}\n",
        "DIGIT = {i for i in \"1234567890\"}\n",
        "PUNCT = {i for i in r\".?,!:$£@/-\\\\\"}\n",
        "\n",
        "\n",
        "__NUM__ = \"__NUM__\"  # Numbers\n",
        "__PUN__ = \"__PUN__\"  # Punctuation\n",
        "__MIX__ = \"__MIX__\"  # Mix of numbers and puntuation\n",
        "MY_SPECIALS = [__UNK__, __NUM__, __PUN__, __MIX__]\n",
        "\n",
        "@unknown_wrapped\n",
        "def my_tokenizer(text: str) -> Iterator[Token]:\n",
        "  for word in text.split(\" \"):\n",
        "    # Implement here, use keyword `yield` instead of return to produce an iterator over your tokens\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT_16-BvW63B",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "# Implement the preprocessor described above. Feel free to add other steps in the preprocessing.\n",
        "\n",
        "def regex_preprocess(text: str) -> str:\n",
        "    text = re.sub(r\"(?<=[a-z])([.?,!:])\", r\" \\1\", text.lower())  # Add an extra space around punctuation (usefull in english)\n",
        "    text = re.sub(r\"([.?,!:])(?=[a-z])\", r\"\\1 \", text)           # Add an extra space around punctuation (usefull in english)\n",
        "    return re.sub(r\"[^ a-z0-9.?,!:$£@/\\-\\\\]\", \" \", text)         # Remove any non basic character\n",
        "\n",
        "\n",
        "# Implement the tokenizer described above. Examples of groups of tokens are: numbers, punctuation, mix of those...\n",
        "ALPHA = {i for i in \"azertyuiopqsdfghjklmwxcvbn\"}\n",
        "DIGIT = {i for i in \"1234567890\"}\n",
        "PUNCT = {i for i in r\".?,!:$£@/-\\\\\"}\n",
        "\n",
        "\n",
        "__NUM__ = \"__NUM__\"\n",
        "__PUN__ = \"__PUN__\"\n",
        "__MIX__ = \"__MIX__\"\n",
        "MY_SPECIALS = [__UNK__, __NUM__, __PUN__, __MIX__]\n",
        "\n",
        "\n",
        "@unknown_wrapped\n",
        "def special_tokenizer(text: str) -> Iterator[Token]:\n",
        "    for word in text.split(\" \"):\n",
        "        if not word in STOP_WORD_S:\n",
        "            if all(c in ALPHA for c in word):\n",
        "                yield word\n",
        "            elif all(c in DIGIT for c in word):\n",
        "                yield __NUM__\n",
        "            elif all(c in PUNCT for c in word):\n",
        "                yield __PUN__\n",
        "            else:\n",
        "                yield __MIX__\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1L3v8X_WKvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Same Questions as before but with your new preprocessing and tokenizer\n",
        "\n",
        "# What are the most common tokens\n",
        "\n",
        "# Plot token occurences and cumulated token occurences.\n",
        "\n",
        "# How many percentages of the token are included in the vocabulary if we use 1_000 tokens ? 10_000 ? 100_000 ?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrSkQDgrcRMo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ6QaT1et6Z4",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "# Same Questions as before but with your new preprocessing and tokenizer\n",
        "*_, word_count = compute_vocabulary(x_train, max_size=10**4, tokenize_f=special_tokenizer, specials=MY_SPECIALS, preprocess_f=regex_preprocess)\n",
        "\n",
        "\n",
        "# What are the most common tokens\n",
        "print(list(word_count.most_common(100)))\n",
        "\n",
        "# Plot token occurences and cumulated token occurences.\n",
        "plot_token_count(word_count, n=10000)\n",
        "plot_accumulated_token_count(word_count, n=10000)\n",
        "\n",
        "# How many percentages of the token are included in the vocabulary if we use 1_000 tokens ? 10_000 ? 100_000 ?\n",
        "for size in [10**3, 10**4, 10**5]:\n",
        "  print(f\"With a vocabulary of size {size}, you cover {sum(t for _, t in word_count.most_common(size)) / sum(word_count.values()) * 100:0.2f}% of the encountered tokens\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFHy4rtVW63D",
        "colab_type": "text"
      },
      "source": [
        "## Basic Model: Bag of Words\n",
        "To implement a Bag of Word model, we first need to convert sentences to vector using a CountVectorizer.\n",
        "\n",
        "It basically counts how many times each token appears in a text and put each value at each token's index.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3sJoJLdW63E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5aabd2d0-7791-46d0-8e7c-2ecfda018ffc"
      },
      "source": [
        "VOCABULARY_SIZE = 10**5\n",
        "## Use skleanr's CountVectorizer to implement a vectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html ##\n",
        "# Remember to specify the vocabulary, the tokenizer and the preprocessor with your own to erase sklearn's defaults\n",
        "\n",
        "\n",
        "vocabulary, tokenizer_f, _ = compute_vocabulary(x_train, max_size=VOCABULARY_SIZE, tokenize_f=my__tokenizer, preprocess_f=my_preprocess, specials=MY_SPECIALS)\n",
        "# Create the CountVectorizer here"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With a vocabulary of size 10000, you cover 89.11%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqJIAYkVdXqq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "cellView": "form",
        "outputId": "5b6f4ab0-8adf-4452-b887-7f5847eb0c03"
      },
      "source": [
        "#@title\n",
        "VOCABULARY_SIZE = 10**5\n",
        "## Use skleanr's CountVectorizer to implement a vectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html ##\n",
        "\n",
        "# Remember to specify the vocabulary, the tokenizer and the preprocessor with your own to erase sklearn's defaults\n",
        "\n",
        "vocabulary, tokenizer_f, _ = compute_vocabulary(x_train, max_size=VOCABULARY_SIZE, tokenize_f=special_tokenizer, preprocess_f=regex_preprocess, specials=MY_SPECIALS)\n",
        "vectorizer = sklearn.feature_extraction.text.CountVectorizer(vocabulary=vocabulary, tokenizer=tokenizer_f, preprocessor=regex_preprocess, binary=True)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With a vocabulary of size 100000, you cover 95.66%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsT6QNB7ezHw",
        "colab_type": "text"
      },
      "source": [
        "The count vectorizer should have its features identicall to our vocabulary.\n",
        "\n",
        "Try to preprocess a text and give it to the count vectorizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gW48ask9fcCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Check if your vectorizer is correct with a small sentence ##"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttsLscfKW63G",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "## Check if your vectorizer is correct ##\n",
        "\n",
        "print(vectorizer.get_feature_names() == list(vocabulary.keys()))\n",
        "\n",
        "print(regex_preprocess(\"12 / tobacco.\"))\n",
        "print(vectorizer.fit_transform([\"12 / tobacco.\"]))\n",
        "\n",
        "print(vocabulary[\"tobacco\"])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpxSVOmIgJE7",
        "colab_type": "text"
      },
      "source": [
        "We will now start building our model.\n",
        "\n",
        "You can use any optimizer (`SGD`, `RMSProp`, …) but `Adam` is one of the best currently. It converges faster and to a better minimum than other optimizers most of the times\n",
        "\n",
        "We are doing a classification problem, use `sparse_categorical_crossentropy` as your loss and `sparse_categorical_accuracy` as your metric.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h59cqjJPkhal",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Create a Sequential model that takes a sentence vector in input (size=VOCABULARY_SIZE) and returns a vector of size NUM_CLASSES. ##\n",
        "# Find help here: https://keras.io/models/sequential/\n",
        "# and here: https://www.tensorflow.org/tutorials/keras/classification\n",
        "\n",
        "# Create your model here and compile it.\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iXyfSm0W63J",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "## Create a Sequential model that takes a sentence vector in input (size=VOCABULARY_SIZE) and returns a vector of size NUM_CLASSES. ##\n",
        "# Find help here: https://keras.io/models/sequential/\n",
        "# and here: https://www.tensorflow.org/tutorials/keras/classification\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(input_dim=VOCABULARY_SIZE, units=128, activation=\"relu\"),\n",
        "    keras.layers.Dense(units=32, activation=\"relu\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(units=32, activation=\"relu\"),\n",
        "    keras.layers.Dense(units=NUM_CLASSES, activation=\"softmax\"),\n",
        "])\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"sparse_categorical_accuracy\"])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCxTv2qDlXyc",
        "colab_type": "text"
      },
      "source": [
        "We are now ready to train our model !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Msu69TKbW63L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(vectorizer.fit_transform(x_train), np.array(y_train), epochs=10, batch_size=256, validation_split=0.1, shuffle=True, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFJ4aKD_nZ6B",
        "colab_type": "text"
      },
      "source": [
        "We can also evaluate our model on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTrjVoYWW63N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "d36735eb-0950-441f-a184-5e53af315b71"
      },
      "source": [
        "model.evaluate(vectorizer.fit_transform(x_test), y_test, verbose=2)\n",
        "\n",
        "y_pred = model.predict_classes(vectorizer.fit_transform(x_test))\n",
        "\n",
        "print(pd.DataFrame(sklearn.metrics.confusion_matrix(y_test, y_pred), columns=CLASS_NAMES))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'scipy.sparse.csr.csr_matrix'>, <class 'NoneType'>\n",
            "3323/3323 - 3s - loss: 0.3964 - sparse_categorical_accuracy: 0.8703\n",
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'scipy.sparse.csr.csr_matrix'>, <class 'NoneType'>\n",
            "   form  email  handwritten  advertisement  invoice\n",
            "0   538     19           31             53       43\n",
            "1    17    634            3              8        3\n",
            "2    13      5          514            121       14\n",
            "3     5      2           34            622        7\n",
            "4    10      2           19             22      584\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoTPVaKYW63Q",
        "colab_type": "text"
      },
      "source": [
        "# A bit more complex: Recurrent Neural Networks and Long-Short Term Memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2z0k5J1W63R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "5188520c-e649-4005-bb02-36936bf283fc"
      },
      "source": [
        "VOCABULARY_SIZE = 10**4\n",
        "EMBEDDING_SIZE = 64\n",
        "MAX_SEQ_LEN = 1 * 10**2\n",
        "\n",
        "__PAD__ = \"__PAD__\"\n",
        "PAD_SPECIALS = MY_SPECIALS + [__PAD__]\n",
        "\n",
        "vocabulary, tokenizer_f, _ = compute_vocabulary(x_train, max_size=VOCABULARY_SIZE, tokenize_f=special_tokenizer, preprocess_f=regex_preprocess, specials=PAD_SPECIALS)\n",
        "\n",
        "def build_vectorizer(vocabulary, tokenizer_f, preprocess_f):\n",
        "    def rnn_vectorizer(x: List[InputText]):\n",
        "        return [[vocabulary[token] for token in tokenizer_f(preprocess_f(text))] for text in x]\n",
        "    return rnn_vectorizer\n",
        "\n",
        "vectorizer = build_vectorizer(vocabulary, tokenizer_f, regex_preprocess)\n",
        "\n",
        "print(vectorizer(x_test[0:2]), sep=\"\\n\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With a vocabulary of size 10000, you cover 91.00%\n",
            "[[0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 5, 0, 0, 0, 0, 0, 0, 4, 2, 152, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 2, 0, 0, 2, 0, 8, 0, 0, 4, 0, 0, 2, 0, 0, 0, 0, 0, 0, 203, 0, 0, 0, 2, 3, 11, 0, 0, 2, 0, 2, 0], [13, 4, 0, 0, 0, 3, 2, 3, 2, 0, 20, 2, 3, 3, 2, 3, 2, 0, 0, 0, 4, 2, 0, 2, 13, 3, 3, 3, 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBaJ242hAAK5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pad_f = partial(keras.preprocessing.sequence.pad_sequences, maxlen=MAX_SEQ_LEN, padding=\"post\", truncating=\"post\", value=vocabulary[__PAD__])\n",
        "\n",
        "rnn_x_train, rnn_x_test = map(pad_f, map(vectorizer, [x_train, x_test]))\n",
        "rnn_y_train, rnn_y_test = map(np.array, [y_train, y_test])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fnoxz3yBW63T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "725736fb-5c73-4791-d7ed-c4db6aab3380"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Embedding(VOCABULARY_SIZE, EMBEDDING_SIZE, input_length=MAX_SEQ_LEN),\n",
        "    keras.layers.Bidirectional(keras.layers.GRU(128, return_sequences=True)),\n",
        "    keras.layers.Bidirectional(keras.layers.GRU(128, dropout=0.5)),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(units=NUM_CLASSES, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.005)\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"sparse_categorical_accuracy\"])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 100, 64)           640000    \n",
            "_________________________________________________________________\n",
            "bidirectional_9 (Bidirection (None, 100, 256)          148992    \n",
            "_________________________________________________________________\n",
            "bidirectional_10 (Bidirectio (None, 256)               296448    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 5)                 1285      \n",
            "=================================================================\n",
            "Total params: 1,086,725\n",
            "Trainable params: 1,086,725\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnBGJz3-W63V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "1456916b-07a0-4c1c-a577-386ed02194a1"
      },
      "source": [
        "model.fit(rnn_x_train, rnn_y_train, epochs=10, batch_size=128, validation_split=0.1, verbose=1, shuffle=True)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11656 samples, validate on 1296 samples\n",
            "Epoch 1/10\n",
            "11656/11656 [==============================] - 21s 2ms/sample - loss: 1.0240 - sparse_categorical_accuracy: 0.5982 - val_loss: 1.2492 - val_sparse_categorical_accuracy: 0.5170\n",
            "Epoch 2/10\n",
            "11656/11656 [==============================] - 11s 952us/sample - loss: 0.4910 - sparse_categorical_accuracy: 0.8306 - val_loss: 0.7508 - val_sparse_categorical_accuracy: 0.7600\n",
            "Epoch 3/10\n",
            "11656/11656 [==============================] - 11s 911us/sample - loss: 0.3435 - sparse_categorical_accuracy: 0.8825 - val_loss: 0.7548 - val_sparse_categorical_accuracy: 0.7562\n",
            "Epoch 4/10\n",
            "11656/11656 [==============================] - 11s 906us/sample - loss: 0.2479 - sparse_categorical_accuracy: 0.9130 - val_loss: 0.7084 - val_sparse_categorical_accuracy: 0.7948\n",
            "Epoch 5/10\n",
            "11656/11656 [==============================] - 11s 909us/sample - loss: 0.2004 - sparse_categorical_accuracy: 0.9310 - val_loss: 0.8107 - val_sparse_categorical_accuracy: 0.7909\n",
            "Epoch 6/10\n",
            "11656/11656 [==============================] - 11s 918us/sample - loss: 0.1656 - sparse_categorical_accuracy: 0.9411 - val_loss: 1.1894 - val_sparse_categorical_accuracy: 0.7238\n",
            "Epoch 7/10\n",
            "11656/11656 [==============================] - 11s 923us/sample - loss: 0.1598 - sparse_categorical_accuracy: 0.9439 - val_loss: 1.8822 - val_sparse_categorical_accuracy: 0.5965\n",
            "Epoch 8/10\n",
            "11656/11656 [==============================] - 11s 914us/sample - loss: 0.1389 - sparse_categorical_accuracy: 0.9536 - val_loss: 1.6535 - val_sparse_categorical_accuracy: 0.6543\n",
            "Epoch 9/10\n",
            "11656/11656 [==============================] - 11s 921us/sample - loss: 0.1258 - sparse_categorical_accuracy: 0.9562 - val_loss: 1.8741 - val_sparse_categorical_accuracy: 0.6289\n",
            "Epoch 10/10\n",
            "11656/11656 [==============================] - 11s 916us/sample - loss: 0.1198 - sparse_categorical_accuracy: 0.9562 - val_loss: 1.8160 - val_sparse_categorical_accuracy: 0.6914\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fda586b45c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzI58o-TAcHA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "3f8b7ada-39a7-465f-ff8d-0b35f77744c6"
      },
      "source": [
        "model.evaluate(rnn_x_test, rnn_y_test, verbose=2)\n",
        "\n",
        "rnn_pred = model.predict_classes(rnn_x_test)\n",
        "print(class_names)\n",
        "print(sklearn.metrics.confusion_matrix(y_test, rnn_pred))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3323/1 - 4s - loss: 2.3444 - sparse_categorical_accuracy: 0.7951\n",
            "['form', 'email', 'handwritten', 'advertisement', 'invoice']\n",
            "[[459  30  47  55  93]\n",
            " [ 15 628  11   7   4]\n",
            " [ 15  20 457 128  47]\n",
            " [  7  24  45 572  22]\n",
            " [ 30   8  36  37 526]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAtGigynBOx6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "da36e1da-a908-4da4-8420-aedb189cabcc"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Embedding(VOCABULARY_SIZE, EMBEDDING_SIZE, input_length=MAX_SEQ_LEN),\n",
        "    keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True)),\n",
        "    keras_self_attention.SeqSelfAttention(attention_activation='sigmoid'),\n",
        "    keras.layers.Bidirectional(keras.layers.LSTM(128)),\n",
        "    keras.layers.Dense(units=NUM_CLASSES, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.005)\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"sparse_categorical_accuracy\"])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-414a1887d781>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVOCABULARY_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBEDDING_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_SEQ_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mkeras_self_attention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeqSelfAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_activation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'keras_self_attention' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjBa2ANSBUDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(rnn_x_train, rnn_y_train, epochs=10, batch_size=128, validation_split=0.1, verbose=1, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjuJzXibBV_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.evaluate(rnn_x_test, rnn_y_test, verbose=2)\n",
        "\n",
        "rnn_pred = model.predict_classes(rnn_x_test)\n",
        "print(class_names)\n",
        "print(sklearn.metrics.confusion_matrix(y_test, rnn_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}